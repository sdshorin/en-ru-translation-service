num_epochs: 50
gradient_clip_val: 1.0
accumulate_grad_batches: 2

optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  weight_decay: 0.01
  eps: 1e-8
  betas: [0.9, 0.98]

scheduler_steps_parameter: num_training_steps
scheduler:
  _target_: transformers.get_linear_schedule_with_warmup
  num_warmup_steps: 4000
  

# criterion:
#   _target_: torch.nn.CrossEntropyLoss
#   label_smoothing: 0.1
#   ignore_index: ${model.pad_token_id}

early_stopping:
  monitor: "val_loss"
  patience: 5
  mode: "min"
  min_delta: 0.0001

checkpointing:
  dirpath: "checkpoints"
  filename: "transformer-{epoch:02d}-{val_loss:.3f}"
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3
  save_last: true

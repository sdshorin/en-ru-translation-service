num_epochs: 20
gradient_clip_val: 1.0
accumulate_grad_batches: 1

optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
  weight_decay: 0.0001
  eps: 1e-8
  betas: [0.9, 0.999]

scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: 0.005
  pct_start: 0.3
  anneal_strategy: "cos"
  cycle_momentum: true

early_stopping:
  monitor: "val_loss"
  patience: 3
  mode: "min"
  min_delta: 0.001

checkpointing:
  dirpath: "checkpoints"
  filename: "model-{epoch:02d}-{val_loss:.2f}"
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3
  save_last: true